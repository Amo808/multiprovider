{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d926a14",
   "metadata": {},
   "source": [
    "# Memory-RAG и Chunking стратегии с LangChain\n",
    "\n",
    "Этот ноутбук демонстрирует реализацию и сравнение различных стратегий разбиения текста (chunking) для Retrieval-Augmented Generation (RAG) и интеграцию ContextualCompressionRetriever и Memory-RAG пайплайна.\n",
    "\n",
    "Outline:\n",
    "1. Импорт и установка необходимых библиотек\n",
    "2. Создание тестового документа\n",
    "3. Fixed-Size Chunking\n",
    "4. Semantic Chunking\n",
    "5. Recursive Code Chunking\n",
    "6. Adaptive Chunking\n",
    "7. Context-Enriched Chunking\n",
    "8. AI-Driven Chunking (mock)\n",
    "9. Сравнительная оценка стратегий\n",
    "10. ContextualCompressionRetriever\n",
    "11. Memory-RAG пайплайн\n",
    "12. Визуализация метрик\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6c0866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Импорт и установка необходимых библиотек\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "required = [\n",
    "    'langchain', 'langchain-text-splitters', 'sentence-transformers', 'nltk', 'pydantic', 'numpy', 'pandas', 'matplotlib'\n",
    "]\n",
    "\n",
    "for pkg in required:\n",
    "    try:\n",
    "        importlib.import_module(pkg.split('-')[0])\n",
    "    except ImportError:\n",
    "        print(f'Installing {pkg}...')\n",
    "        subprocess.run([sys.executable, '-m', 'pip', 'install', pkg])\n",
    "\n",
    "import re, math, json, time, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "print('Environment ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6bc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Создание тестового документа\n",
    "\n",
    "def create_dummy_document(paragraphs: int = 40) -> str:\n",
    "    base_paras = []\n",
    "    topics = [\n",
    "        'Machine Learning overview and its branches including supervised learning.',\n",
    "        'Neural networks fundamentals: activation functions, backpropagation, optimization.',\n",
    "        'Large Language Models and transformer attention scaling strategies.',\n",
    "        'Vector databases, hybrid retrieval, reranking pipelines and HNSW indexing.',\n",
    "        'Context compression methods: MMR, clustering, summarization with LLMs.',\n",
    "        'Reinforcement Learning from Human Feedback and policy optimization.',\n",
    "        'Multimodal learning: text + image fusion mechanisms and embeddings.',\n",
    "        'Federated learning privacy constraints and differential privacy.',\n",
    "        'Time series forecasting and anomaly detection approaches.',\n",
    "        'Graph algorithms including PageRank and Personalized PageRank for KG.',\n",
    "    ]\n",
    "    for i in range(paragraphs):\n",
    "        topic = topics[i % len(topics)]\n",
    "        base_paras.append(f\"## Section {i+1}\\n\" + topic + ' ' + ' '.join([\n",
    "            'This paragraph elaborates on ' + random.choice(topics).lower(),\n",
    "            'It provides additional clarifications and comparisons.',\n",
    "            'Important for retrieval quality and semantic integrity.'\n",
    "        ]))\n",
    "    return '\\n\\n'.join(base_paras)\n",
    "\n",
    "document = create_dummy_document()\n",
    "print('Document length (chars):', len(document))\n",
    "print(document[:500] + '...')  # preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff01b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Fixed-Size Chunking\n",
    "\n",
    "fixed_splitter = CharacterTextSplitter(\n",
    "    separator='\\n\\n',\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=160,\n",
    "    length_function=len\n",
    ")\n",
    "fixed_chunks = fixed_splitter.split_text(document)\n",
    "fixed_docs = [Document(page_content=c, metadata={'chunk_id': i, 'chunk_type': 'fixed', 'size': len(c)}) for i, c in enumerate(fixed_chunks)]\n",
    "print('Fixed chunks count:', len(fixed_docs))\n",
    "print('Example chunk preview:\\n', fixed_docs[len(fixed_docs)//2].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88213484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Semantic Chunking\n",
    "\n",
    "semantic_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=['\\n## ', '\\n\\n', '\\n', '. ', ' ', ''],\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=120,\n",
    "    length_function=len\n",
    ")\n",
    "semantic_chunks = semantic_splitter.split_text(document)\n",
    "section_pattern = re.compile(r'^##\\s+Section\\s+(\\d+)', re.MULTILINE)\n",
    "semantic_docs = []\n",
    "current_section = None\n",
    "for i, ch in enumerate(semantic_chunks):\n",
    "    m = section_pattern.search(ch)\n",
    "    if m:\n",
    "        current_section = m.group(1)\n",
    "    semantic_docs.append(Document(page_content=ch, metadata={\n",
    "        'chunk_id': i,\n",
    "        'chunk_type': 'semantic',\n",
    "        'section': current_section,\n",
    "        'size': len(ch)\n",
    "    }))\n",
    "print('Semantic chunks count:', len(semantic_docs))\n",
    "print('Example semantic chunk metadata:', semantic_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227bd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Recursive Code Chunking (mock Python code)\n",
    "code_sample = \"\"\"\n",
    "import math\n",
    "\n",
    "class VectorOps:\n",
    "    def norm(self, v):\n",
    "        return math.sqrt(sum(x*x for x in v))\n",
    "\n",
    "    def cosine(self, a, b):\n",
    "        na = self.norm(a); nb = self.norm(b)\n",
    "        return sum(x*y for x,y in zip(a,b)) / (na*nb)\n",
    "\n",
    "def embed(text):\n",
    "    # mock embedding\n",
    "    return [float(len(text)) % 10, len(text)%7, len(text)%5]\n",
    "\n",
    "\"\"\"\n",
    "code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language='python',\n",
    "    chunk_size=120,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "code_chunks = code_splitter.split_text(code_sample)\n",
    "code_docs = [Document(page_content=c, metadata={'chunk_type': 'code', 'chunk_id': i, 'size': len(c)}) for i,c in enumerate(code_chunks)]\n",
    "print('Code chunks:', len(code_docs))\n",
    "for d in code_docs:\n",
    "    print(d.metadata, '=>', d.page_content.replace('\\n',' ')[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Adaptive Chunking\n",
    "from langchain_text_splitters import TextSplitter\n",
    "\n",
    "class AdaptiveTextSplitter(TextSplitter):\n",
    "    def __init__(self, min_chunk_size=300, max_chunk_size=900, min_overlap=40, max_overlap=140, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_chunk_size=min_chunk_size\n",
    "        self.max_chunk_size=max_chunk_size\n",
    "        self.min_overlap=min_overlap\n",
    "        self.max_overlap=max_overlap\n",
    "\n",
    "    def complexity(self, text: str) -> float:\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        if not words:\n",
    "            return 0.0\n",
    "        unique = len(set(words))\n",
    "        density = unique/len(words)\n",
    "        avg_sentence_len = sum(len(s) for s in re.split(r'[.!?]', text) if s.strip()) / max(1, len(re.split(r'[.!?]', text)))\n",
    "        # normalize approx\n",
    "        return min(1.0, 0.5*density + 0.5*min(1.0, avg_sentence_len/180))\n",
    "\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        chunks=[]; current=[]; current_size=0; current_complexity=0.5\n",
    "        for sent in sentences:\n",
    "            c = self.complexity(sent)\n",
    "            current_complexity = (current_complexity + c)/2 if current else c\n",
    "            target = self.max_chunk_size - current_complexity*(self.max_chunk_size-self.min_chunk_size)\n",
    "            if current_size + len(sent) > target and current:\n",
    "                chunks.append(' '.join(current))\n",
    "                # overlap\n",
    "                overlap_target = self.min_overlap + current_complexity*(self.max_overlap-self.min_overlap)\n",
    "                overlap=[]; o_size=0\n",
    "                for s in reversed(current):\n",
    "                    if o_size + len(s) <= overlap_target:\n",
    "                        overlap.insert(0,s); o_size += len(s)\n",
    "                    else:\n",
    "                        break\n",
    "                current = overlap + [sent]\n",
    "                current_size = sum(len(x) for x in current)\n",
    "            else:\n",
    "                current.append(sent); current_size += len(sent)\n",
    "        if current:\n",
    "            chunks.append(' '.join(current))\n",
    "        return chunks\n",
    "\n",
    "adaptive_splitter = AdaptiveTextSplitter()\n",
    "adaptive_chunks = adaptive_splitter.split_text(document)\n",
    "adaptive_docs = [Document(page_content=c, metadata={'chunk_type':'adaptive','chunk_id':i,'size':len(c)}) for i,c in enumerate(adaptive_chunks)]\n",
    "print('Adaptive chunks:', len(adaptive_docs))\n",
    "print('Example adaptive chunk size:', adaptive_docs[0].metadata['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b60a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Context-Enriched Chunking (mock summaries)\n",
    "\n",
    "base_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=['\\n\\n','\\n','. ',' ','']\n",
    ")\n",
    "base_chunks = base_splitter.split_text(document)\n",
    "\n",
    "def mock_summary(text: str) -> str:\n",
    "    first_sentence = text.split('.')[0]\n",
    "    return 'Summary: ' + first_sentence[:120] + '...'\n",
    "\n",
    "enriched_docs = []\n",
    "window_size = 1\n",
    "for i, ch in enumerate(base_chunks):\n",
    "    window_start = max(0, i-window_size)\n",
    "    window_end = min(len(base_chunks), i+window_size+1)\n",
    "    context_parts = [base_chunks[j] for j in range(window_start, window_end) if j!=i]\n",
    "    context_text = ' '.join(context_parts)\n",
    "    summary = mock_summary(context_text) if context_text else ''\n",
    "    enriched_docs.append(Document(page_content=f'Context: {summary}\\n\\nContent: {ch}', metadata={\n",
    "        'chunk_id': i,\n",
    "        'chunk_type': 'context_enriched',\n",
    "        'has_context': bool(context_text)\n",
    "    }))\n",
    "print('Context-enriched chunks:', len(enriched_docs))\n",
    "print('Example enriched doc:\\n', enriched_docs[0].page_content[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc71c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. AI-Driven Chunking (mock)\n",
    "\n",
    "def ai_driven_chunking_mock(text: str, max_chunks: int = 15) -> list[str]:\n",
    "    paragraphs = text.split('\\n\\n')\n",
    "    chunks=[]; current=''\n",
    "    for p in paragraphs:\n",
    "        if len(current)+len(p) < 700:\n",
    "            current += p + '\\n\\n'\n",
    "        else:\n",
    "            if current:\n",
    "                chunks.append(current.strip())\n",
    "            current = p + '\\n\\n'\n",
    "    if current:\n",
    "        chunks.append(current.strip())\n",
    "    # reduce if too many\n",
    "    if len(chunks) > max_chunks:\n",
    "        grouped=[]; group_size = math.ceil(len(chunks)/max_chunks)\n",
    "        for i in range(0,len(chunks),group_size):\n",
    "            grouped.append('\\n\\n'.join(chunks[i:i+group_size]))\n",
    "        chunks = grouped\n",
    "    return chunks\n",
    "\n",
    "ai_chunks = ai_driven_chunking_mock(document, max_chunks=12)\n",
    "ai_docs = [Document(page_content=c, metadata={'chunk_id':i,'chunk_type':'ai_mock','size':len(c)}) for i,c in enumerate(ai_chunks)]\n",
    "print('AI-driven mock chunks:', len(ai_docs))\n",
    "print('Example AI chunk size:', ai_docs[0].metadata['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de651786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Сравнительная оценка стратегий\n",
    "\n",
    "strategies = {\n",
    "    'fixed': fixed_docs,\n",
    "    'semantic': semantic_docs,\n",
    "    'adaptive': adaptive_docs,\n",
    "    'context_enriched': enriched_docs,\n",
    "    'ai_mock': ai_docs\n",
    "}\n",
    "\n",
    "keywords = ['machine', 'learning', 'vector', 'retrieval', 'compression', 'graph']\n",
    "phrases = ['Machine Learning overview', 'Large Language Models', 'Personalized PageRank']\n",
    "\n",
    "def keyword_coverage(docs):\n",
    "    texts = [d.page_content.lower() for d in docs]\n",
    "    found=0\n",
    "    for kw in keywords:\n",
    "        if any(kw in t for t in texts):\n",
    "            found += 1\n",
    "    return found / len(keywords)\n",
    "\n",
    "def average_chunk_size(docs):\n",
    "    return sum(len(d.page_content) for d in docs)/len(docs)\n",
    "\n",
    "def size_std(docs):\n",
    "    avg = average_chunk_size(docs)\n",
    "    return (sum((len(d.page_content)-avg)**2 for d in docs)/len(docs))**0.5\n",
    "\n",
    "def coherence(docs):\n",
    "    # crude: penalize chunks starting lowercase or ending without punctuation\n",
    "    penalty=0\n",
    "    for d in docs:\n",
    "        txt=d.page_content.strip()\n",
    "        if txt and (txt[0].islower() or txt[0] in ',;:)]}'):\n",
    "            penalty+=1\n",
    "        if txt and not re.search(r'[.!?]$', txt):\n",
    "            penalty+=1\n",
    "    max_pen = 2*len(docs)\n",
    "    return 1 - penalty/max_pen\n",
    "\n",
    "results=[]\n",
    "for name, ds in strategies.items():\n",
    "    results.append({\n",
    "        'strategy': name,\n",
    "        'chunks': len(ds),\n",
    "        'keyword_coverage': round(keyword_coverage(ds),2),\n",
    "        'avg_size': round(average_chunk_size(ds),1),\n",
    "        'size_std': round(size_std(ds),1),\n",
    "        'coherence': round(coherence(ds),2)\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(results_df['strategy'], results_df['keyword_coverage'])\n",
    "plt.title('Keyword Coverage')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(results_df['strategy'], results_df['coherence'])\n",
    "plt.title('Coherence Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29cd548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. ContextualCompressionRetriever (mock/simple pipeline)\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.schema import BaseDocumentTransformer\n",
    "\n",
    "class TopKCompressor(BaseDocumentTransformer):\n",
    "    def __init__(self, max_chars: int = 400):\n",
    "        self.max_chars = max_chars\n",
    "    def transform_documents(self, documents, **kwargs):\n",
    "        out=[]\n",
    "        for d in documents:\n",
    "            content = d.page_content\n",
    "            if len(content) > self.max_chars:\n",
    "                content = content[:self.max_chars] + '...'\n",
    "            out.append(Document(page_content=content, metadata={**d.metadata, 'compressed': True}))\n",
    "        return out\n",
    "\n",
    "# base retriever mock: just returns first N semantic docs\n",
    "class MockRetriever:\n",
    "    def invoke(self, query: str):\n",
    "        # pretend ranking by simple keyword matching\n",
    "        ranked = []\n",
    "        for d in semantic_docs:\n",
    "            score = sum(query.lower().count(kw) for kw in ['learning','vector','graph'])\n",
    "            ranked.append((score, d))\n",
    "        ranked.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [d for _,d in ranked[:8]]\n",
    "\n",
    "base_retriever = MockRetriever()\n",
    "compressor = TopKCompressor(max_chars=300)\n",
    "compressed = compressor.transform_documents(base_retriever.invoke('graph learning vector'))\n",
    "print('Compressed docs returned:', len(compressed))\n",
    "print('Sample compressed content length:', len(compressed[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a368be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Memory-RAG пайплайн (упрощенный)\n",
    "# Steps: ingest -> chunk -> embed -> store -> retrieve -> compress -> generate\n",
    "\n",
    "# 1. Ingest (already have 'document')\n",
    "source_id = 'dummy-doc-001'\n",
    "\n",
    "# 2. Chunk (choose adaptive for demo)\n",
    "ingested_chunks = adaptive_docs\n",
    "\n",
    "# 3. Embeddings (sentence-transformers)\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = embed_model.encode([d.page_content for d in ingested_chunks])\n",
    "\n",
    "# 4. Store (in-memory vector index)\n",
    "index = [(emb, d) for emb, d in zip(embeddings, ingested_chunks)]\n",
    "\n",
    "# simple cosine\n",
    "def cosine(a,b):\n",
    "    return float(np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "\n",
    "# 5. Retrieve\n",
    "query = 'Explain transformer attention scaling strategies and vector databases'\n",
    "q_emb = embed_model.encode([query])[0]\n",
    "scored = [(cosine(q_emb, emb), d) for emb,d in index]\n",
    "scored.sort(key=lambda x: x[0], reverse=True)\n",
    "retrieved = [d for _,d in scored[:5]]\n",
    "\n",
    "# 6. Compress (reuse compressor)\n",
    "compressed_retrieved = compressor.transform_documents(retrieved)\n",
    "\n",
    "# 7. Generate (mock generation combining compressed text)\n",
    "answer_context = '\\n\\n'.join(d.page_content for d in compressed_retrieved)\n",
    "mock_answer = 'Answer:\\n' + answer_context[:1000] + '\\n...[truncated]'\n",
    "print(mock_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Визуализация дополнительных метрик\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(results_df['strategy'], results_df['avg_size'])\n",
    "plt.title('Average Chunk Size')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(results_df['strategy'], results_df['size_std'])\n",
    "plt.title('Chunk Size StdDev')\n",
    "plt.show()\n",
    "\n",
    "print('Finished all sections.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d764e8e1",
   "metadata": {},
   "source": [
    "# 13. Chat Integration с автосжатием контекста (RAG + LangChain)\n",
    "\n",
    "Этот раздел демонстрирует:\n",
    "- Интеграцию LangChain в чат-пайплайн\n",
    "- Автоматическое сжатие контекста при каждом запросе\n",
    "- Функции для управления порядком сообщений в JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8127da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.1 ChatRAGCompressor — интеграция LangChain с автосжатием контекста\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "@dataclass\n",
    "class ChatMessage:\n",
    "    \"\"\"Структура сообщения чата\"\"\"\n",
    "    role: str  # 'user', 'assistant', 'system'\n",
    "    content: str\n",
    "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
    "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
    "    compressed: bool = False\n",
    "    original_length: int = 0\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'role': self.role,\n",
    "            'content': self.content,\n",
    "            'timestamp': self.timestamp,\n",
    "            'metadata': self.metadata,\n",
    "            'compressed': self.compressed,\n",
    "            'original_length': self.original_length\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_dict(cls, data: Dict) -> 'ChatMessage':\n",
    "        return cls(**data)\n",
    "\n",
    "\n",
    "class ChatRAGCompressor:\n",
    "    \"\"\"\n",
    "    Интеграция LangChain + RAG с автоматическим сжатием контекста.\n",
    "    При каждом запросе:\n",
    "    1. Сжимает старые сообщения\n",
    "    2. Извлекает релевантный контекст через RAG\n",
    "    3. Формирует оптимальный промпт\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_model_name: str = 'all-MiniLM-L6-v2',\n",
    "        max_context_tokens: int = 4000,\n",
    "        compression_threshold: int = 500,\n",
    "        keep_recent_messages: int = 3\n",
    "    ):\n",
    "        self.embed_model = SentenceTransformer(embed_model_name)\n",
    "        self.max_context_tokens = max_context_tokens\n",
    "        self.compression_threshold = compression_threshold\n",
    "        self.keep_recent_messages = keep_recent_messages\n",
    "        \n",
    "        # История чата\n",
    "        self.messages: List[ChatMessage] = []\n",
    "        # Индекс для RAG (embeddings + messages)\n",
    "        self.message_index: List[tuple] = []\n",
    "        \n",
    "    def _estimate_tokens(self, text: str) -> int:\n",
    "        \"\"\"Примерная оценка токенов (4 символа ~ 1 токен)\"\"\"\n",
    "        return len(text) // 4\n",
    "    \n",
    "    def _compress_message(self, message: ChatMessage) -> ChatMessage:\n",
    "        \"\"\"Сжатие одного сообщения\"\"\"\n",
    "        content = message.content\n",
    "        original_length = len(content)\n",
    "        \n",
    "        if len(content) <= self.compression_threshold:\n",
    "            return message\n",
    "        \n",
    "        # Стратегии сжатия:\n",
    "        # 1. Удаление приветствий и филлеров\n",
    "        fillers = [\n",
    "            r'\\b(um|uh|well|so|basically|actually|you know|i mean)\\b',\n",
    "            r'^(hi|hello|hey|greetings)[,!.\\s]*',\n",
    "            r'(thanks|thank you)[,!.\\s]*$',\n",
    "        ]\n",
    "        compressed = content\n",
    "        for pattern in fillers:\n",
    "            compressed = re.sub(pattern, '', compressed, flags=re.IGNORECASE)\n",
    "        \n",
    "        # 2. Удаление повторяющихся пробелов\n",
    "        compressed = re.sub(r'\\s+', ' ', compressed).strip()\n",
    "        \n",
    "        # 3. Если всё ещё большое — обрезаем до ключевых предложений\n",
    "        if len(compressed) > self.compression_threshold:\n",
    "            sentences = nltk.sent_tokenize(compressed)\n",
    "            # Берём первое и последнее предложения + ключевые\n",
    "            if len(sentences) > 3:\n",
    "                compressed = f\"{sentences[0]} [...] {sentences[-1]}\"\n",
    "        \n",
    "        new_msg = ChatMessage(\n",
    "            role=message.role,\n",
    "            content=compressed,\n",
    "            timestamp=message.timestamp,\n",
    "            metadata={**message.metadata, 'compression_ratio': len(compressed)/original_length},\n",
    "            compressed=True,\n",
    "            original_length=original_length\n",
    "        )\n",
    "        return new_msg\n",
    "    \n",
    "    def _embed_message(self, message: ChatMessage) -> np.ndarray:\n",
    "        \"\"\"Создание эмбеддинга для сообщения\"\"\"\n",
    "        return self.embed_model.encode([message.content])[0]\n",
    "    \n",
    "    def add_message(self, role: str, content: str, metadata: Dict = None):\n",
    "        \"\"\"Добавление нового сообщения в чат\"\"\"\n",
    "        msg = ChatMessage(\n",
    "            role=role,\n",
    "            content=content,\n",
    "            metadata=metadata or {},\n",
    "            original_length=len(content)\n",
    "        )\n",
    "        self.messages.append(msg)\n",
    "        \n",
    "        # Добавляем в RAG индекс\n",
    "        emb = self._embed_message(msg)\n",
    "        self.message_index.append((emb, msg, len(self.messages) - 1))\n",
    "        \n",
    "    def _retrieve_relevant_context(self, query: str, top_k: int = 5) -> List[ChatMessage]:\n",
    "        \"\"\"RAG: извлечение релевантных сообщений по запросу\"\"\"\n",
    "        if not self.message_index:\n",
    "            return []\n",
    "        \n",
    "        q_emb = self.embed_model.encode([query])[0]\n",
    "        scored = []\n",
    "        for emb, msg, idx in self.message_index:\n",
    "            score = cosine(q_emb, emb)\n",
    "            scored.append((score, msg, idx))\n",
    "        \n",
    "        scored.sort(key=lambda x: x[0], reverse=True)\n",
    "        return [msg for _, msg, _ in scored[:top_k]]\n",
    "    \n",
    "    def build_context(self, current_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Построение оптимального контекста для LLM:\n",
    "        1. Последние N сообщений (без сжатия)\n",
    "        2. Релевантные старые сообщения (сжатые через RAG)\n",
    "        3. System prompt\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'system': '',\n",
    "            'context_messages': [],\n",
    "            'recent_messages': [],\n",
    "            'current_query': current_query,\n",
    "            'stats': {}\n",
    "        }\n",
    "        \n",
    "        total_tokens = 0\n",
    "        \n",
    "        # 1. Недавние сообщения (без сжатия)\n",
    "        recent = self.messages[-self.keep_recent_messages:] if len(self.messages) > self.keep_recent_messages else self.messages\n",
    "        for msg in recent:\n",
    "            result['recent_messages'].append(msg.to_dict())\n",
    "            total_tokens += self._estimate_tokens(msg.content)\n",
    "        \n",
    "        # 2. RAG: релевантные старые сообщения (сжатые)\n",
    "        if len(self.messages) > self.keep_recent_messages:\n",
    "            relevant = self._retrieve_relevant_context(current_query, top_k=5)\n",
    "            for msg in relevant:\n",
    "                if msg not in recent:\n",
    "                    compressed_msg = self._compress_message(msg)\n",
    "                    tokens = self._estimate_tokens(compressed_msg.content)\n",
    "                    if total_tokens + tokens < self.max_context_tokens:\n",
    "                        result['context_messages'].append(compressed_msg.to_dict())\n",
    "                        total_tokens += tokens\n",
    "        \n",
    "        result['stats'] = {\n",
    "            'total_tokens_estimate': total_tokens,\n",
    "            'recent_count': len(result['recent_messages']),\n",
    "            'context_count': len(result['context_messages']),\n",
    "            'compression_applied': any(m.get('compressed') for m in result['context_messages'])\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_messages_json(self) -> str:\n",
    "        \"\"\"Экспорт всех сообщений в JSON\"\"\"\n",
    "        return json.dumps([m.to_dict() for m in self.messages], ensure_ascii=False, indent=2)\n",
    "    \n",
    "    def load_messages_json(self, json_str: str):\n",
    "        \"\"\"Импорт сообщений из JSON\"\"\"\n",
    "        data = json.loads(json_str)\n",
    "        self.messages = [ChatMessage.from_dict(d) for d in data]\n",
    "        # Перестроить индекс\n",
    "        self.message_index = []\n",
    "        for i, msg in enumerate(self.messages):\n",
    "            emb = self._embed_message(msg)\n",
    "            self.message_index.append((emb, msg, i))\n",
    "\n",
    "\n",
    "# Демонстрация\n",
    "chat = ChatRAGCompressor(max_context_tokens=2000, keep_recent_messages=3)\n",
    "\n",
    "# Добавляем тестовые сообщения\n",
    "chat.add_message('user', 'Привет! Расскажи мне про машинное обучение и нейронные сети.')\n",
    "chat.add_message('assistant', 'Машинное обучение — это раздел искусственного интеллекта, который позволяет компьютерам обучаться на данных без явного программирования. Нейронные сети — это архитектура, вдохновлённая работой мозга.')\n",
    "chat.add_message('user', 'А что такое трансформеры и attention механизм?')\n",
    "chat.add_message('assistant', 'Трансформеры — это архитектура нейронных сетей, использующая механизм внимания (attention) для обработки последовательностей. Self-attention позволяет модели взвешивать важность разных частей входа.')\n",
    "chat.add_message('user', 'Как работают векторные базы данных?')\n",
    "chat.add_message('assistant', 'Векторные БД хранят эмбеддинги и выполняют быстрый поиск по сходству через алгоритмы типа HNSW или IVF.')\n",
    "\n",
    "print('Всего сообщений:', len(chat.messages))\n",
    "print('\\nПостроение контекста для нового запроса:')\n",
    "context = chat.build_context('Объясни связь между attention и векторным поиском')\n",
    "print(json.dumps(context['stats'], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.2 Функции для управления порядком сообщений в JSON\n",
    "\n",
    "class ChatMessageManager:\n",
    "    \"\"\"\n",
    "    Утилиты для работы с историей чата:\n",
    "    - Перестановка сообщений\n",
    "    - Перемещение вверх/вниз\n",
    "    - Swap между позициями\n",
    "    - Pin важных сообщений\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, messages: List[Dict] = None):\n",
    "        self.messages = messages or []\n",
    "    \n",
    "    def load_json(self, json_str: str):\n",
    "        \"\"\"Загрузка из JSON строки\"\"\"\n",
    "        self.messages = json.loads(json_str)\n",
    "        return self\n",
    "    \n",
    "    def load_list(self, messages: List[Dict]):\n",
    "        \"\"\"Загрузка из списка\"\"\"\n",
    "        self.messages = copy.deepcopy(messages)\n",
    "        return self\n",
    "    \n",
    "    def to_json(self, indent: int = 2) -> str:\n",
    "        \"\"\"Экспорт в JSON\"\"\"\n",
    "        return json.dumps(self.messages, ensure_ascii=False, indent=indent)\n",
    "    \n",
    "    def to_list(self) -> List[Dict]:\n",
    "        \"\"\"Получить список сообщений\"\"\"\n",
    "        return copy.deepcopy(self.messages)\n",
    "    \n",
    "    # === ПЕРЕСТАНОВКИ ===\n",
    "    \n",
    "    def swap(self, index1: int, index2: int) -> 'ChatMessageManager':\n",
    "        \"\"\"Поменять местами два сообщения по индексам\"\"\"\n",
    "        if 0 <= index1 < len(self.messages) and 0 <= index2 < len(self.messages):\n",
    "            self.messages[index1], self.messages[index2] = self.messages[index2], self.messages[index1]\n",
    "        return self\n",
    "    \n",
    "    def move_up(self, index: int) -> 'ChatMessageManager':\n",
    "        \"\"\"Переместить сообщение на одну позицию вверх\"\"\"\n",
    "        if 0 < index < len(self.messages):\n",
    "            self.swap(index, index - 1)\n",
    "        return self\n",
    "    \n",
    "    def move_down(self, index: int) -> 'ChatMessageManager':\n",
    "        \"\"\"Переместить сообщение на одну позицию вниз\"\"\"\n",
    "        if 0 <= index < len(self.messages) - 1:\n",
    "            self.swap(index, index + 1)\n",
    "        return self\n",
    "    \n",
    "    def move_to_top(self, index: int) -> 'ChatMessageManager':\n",
    "        \"\"\"Переместить сообщение в начало\"\"\"\n",
    "        if 0 < index < len(self.messages):\n",
    "            msg = self.messages.pop(index)\n",
    "            self.messages.insert(0, msg)\n",
    "        return self\n",
    "    \n",
    "    def move_to_bottom(self, index: int) -> 'ChatMessageManager':\n",
    "        \"\"\"Переместить сообщение в конец\"\"\"\n",
    "        if 0 <= index < len(self.messages) - 1:\n",
    "            msg = self.messages.pop(index)\n",
    "            self.messages.append(msg)\n",
    "        return self\n",
    "    \n",
    "    def move_to_position(self, from_index: int, to_index: int) -> 'ChatMessageManager':\n",
    "        \"\"\"Переместить сообщение на конкретную позицию\"\"\"\n",
    "        if 0 <= from_index < len(self.messages) and 0 <= to_index < len(self.messages):\n",
    "            msg = self.messages.pop(from_index)\n",
    "            self.messages.insert(to_index, msg)\n",
    "        return self\n",
    "    \n",
    "    def reverse(self) -> 'ChatMessageManager':\n",
    "        \"\"\"Развернуть порядок всех сообщений\"\"\"\n",
    "        self.messages.reverse()\n",
    "        return self\n",
    "    \n",
    "    def sort_by_timestamp(self, ascending: bool = True) -> 'ChatMessageManager':\n",
    "        \"\"\"Сортировка по времени\"\"\"\n",
    "        self.messages.sort(\n",
    "            key=lambda m: m.get('timestamp', ''),\n",
    "            reverse=not ascending\n",
    "        )\n",
    "        return self\n",
    "    \n",
    "    def sort_by_role(self, order: List[str] = None) -> 'ChatMessageManager':\n",
    "        \"\"\"Сортировка по роли (system -> user -> assistant)\"\"\"\n",
    "        order = order or ['system', 'user', 'assistant']\n",
    "        role_priority = {r: i for i, r in enumerate(order)}\n",
    "        self.messages.sort(key=lambda m: role_priority.get(m.get('role', ''), 999))\n",
    "        return self\n",
    "    \n",
    "    # === ФИЛЬТРАЦИЯ ===\n",
    "    \n",
    "    def filter_by_role(self, role: str) -> List[Dict]:\n",
    "        \"\"\"Получить сообщения только определённой роли\"\"\"\n",
    "        return [m for m in self.messages if m.get('role') == role]\n",
    "    \n",
    "    def remove_by_index(self, index: int) -> 'ChatMessageManager':\n",
    "        \"\"\"Удалить сообщение по индексу\"\"\"\n",
    "        if 0 <= index < len(self.messages):\n",
    "            self.messages.pop(index)\n",
    "        return self\n",
    "    \n",
    "    def duplicate(self, index: int) -> 'ChatMessageManager':\n",
    "        \"\"\"Дублировать сообщение\"\"\"\n",
    "        if 0 <= index < len(self.messages):\n",
    "            msg_copy = copy.deepcopy(self.messages[index])\n",
    "            msg_copy['timestamp'] = datetime.now().isoformat()\n",
    "            msg_copy['metadata'] = msg_copy.get('metadata', {})\n",
    "            msg_copy['metadata']['duplicated_from'] = index\n",
    "            self.messages.insert(index + 1, msg_copy)\n",
    "        return self\n",
    "    \n",
    "    # === ГРУППОВЫЕ ОПЕРАЦИИ ===\n",
    "    \n",
    "    def group_by_role(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Группировка сообщений по ролям\"\"\"\n",
    "        groups = {}\n",
    "        for msg in self.messages:\n",
    "            role = msg.get('role', 'unknown')\n",
    "            if role not in groups:\n",
    "                groups[role] = []\n",
    "            groups[role].append(msg)\n",
    "        return groups\n",
    "    \n",
    "    def interleave_user_assistant(self) -> 'ChatMessageManager':\n",
    "        \"\"\"\n",
    "        Перестроить: чередовать user/assistant.\n",
    "        Полезно для восстановления диалоговой структуры.\n",
    "        \"\"\"\n",
    "        users = [m for m in self.messages if m.get('role') == 'user']\n",
    "        assistants = [m for m in self.messages if m.get('role') == 'assistant']\n",
    "        others = [m for m in self.messages if m.get('role') not in ('user', 'assistant')]\n",
    "        \n",
    "        result = others[:]  # system messages first\n",
    "        for u, a in zip(users, assistants):\n",
    "            result.append(u)\n",
    "            result.append(a)\n",
    "        # Добавить оставшиеся\n",
    "        result.extend(users[len(assistants):])\n",
    "        result.extend(assistants[len(users):])\n",
    "        \n",
    "        self.messages = result\n",
    "        return self\n",
    "    \n",
    "    def preview(self, max_content_len: int = 50) -> None:\n",
    "        \"\"\"Предпросмотр сообщений\"\"\"\n",
    "        print(f\"{'#':<3} {'Role':<12} {'Content':<{max_content_len+3}} Compressed\")\n",
    "        print('-' * (max_content_len + 25))\n",
    "        for i, m in enumerate(self.messages):\n",
    "            content = m.get('content', '')[:max_content_len]\n",
    "            if len(m.get('content', '')) > max_content_len:\n",
    "                content += '...'\n",
    "            compressed = '✓' if m.get('compressed') else ''\n",
    "            print(f\"{i:<3} {m.get('role', '?'):<12} {content:<{max_content_len+3}} {compressed}\")\n",
    "\n",
    "\n",
    "# === ДЕМОНСТРАЦИЯ ===\n",
    "\n",
    "# Создаём менеджер из существующего чата\n",
    "manager = ChatMessageManager()\n",
    "manager.load_json(chat.get_messages_json())\n",
    "\n",
    "print(\"=== Исходный порядок ===\")\n",
    "manager.preview()\n",
    "\n",
    "print(\"\\n=== После swap(0, 2) ===\")\n",
    "manager.swap(0, 2).preview()\n",
    "\n",
    "print(\"\\n=== После move_to_bottom(0) ===\")\n",
    "manager.move_to_bottom(0).preview()\n",
    "\n",
    "print(\"\\n=== После reverse() ===\")\n",
    "manager.reverse().preview()\n",
    "\n",
    "print(\"\\n=== Экспорт в JSON (первые 500 символов) ===\")\n",
    "print(manager.to_json()[:500] + '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807316bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.3 Полный пример: Chat API с автосжатием и управлением порядком\n",
    "\n",
    "class ChatAPI:\n",
    "    \"\"\"\n",
    "    API-подобный интерфейс для чата с:\n",
    "    - Автосжатием контекста через RAG\n",
    "    - Управлением порядком сообщений\n",
    "    - Экспортом/импортом JSON\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        self.compressor = ChatRAGCompressor(**kwargs)\n",
    "        self.manager = ChatMessageManager()\n",
    "    \n",
    "    def send_message(self, content: str, role: str = 'user') -> Dict:\n",
    "        \"\"\"Отправить сообщение и получить сжатый контекст\"\"\"\n",
    "        self.compressor.add_message(role, content)\n",
    "        context = self.compressor.build_context(content)\n",
    "        return {\n",
    "            'status': 'ok',\n",
    "            'message_count': len(self.compressor.messages),\n",
    "            'context': context\n",
    "        }\n",
    "    \n",
    "    def get_history(self) -> List[Dict]:\n",
    "        \"\"\"Получить всю историю\"\"\"\n",
    "        return [m.to_dict() for m in self.compressor.messages]\n",
    "    \n",
    "    def export_json(self) -> str:\n",
    "        \"\"\"Экспорт в JSON\"\"\"\n",
    "        return self.compressor.get_messages_json()\n",
    "    \n",
    "    def import_json(self, json_str: str):\n",
    "        \"\"\"Импорт из JSON\"\"\"\n",
    "        self.compressor.load_messages_json(json_str)\n",
    "    \n",
    "    # === Управление порядком ===\n",
    "    \n",
    "    def reorder(self, operation: str, **kwargs) -> Dict:\n",
    "        \"\"\"\n",
    "        Универсальный метод для изменения порядка.\n",
    "        \n",
    "        Operations:\n",
    "        - 'swap': swap index1 и index2\n",
    "        - 'move_up': поднять index на 1\n",
    "        - 'move_down': опустить index на 1\n",
    "        - 'move_to': переместить from_index на to_index\n",
    "        - 'reverse': развернуть весь список\n",
    "        - 'sort_time': сортировка по времени\n",
    "        - 'interleave': чередование user/assistant\n",
    "        \"\"\"\n",
    "        self.manager.load_list([m.to_dict() for m in self.compressor.messages])\n",
    "        \n",
    "        if operation == 'swap':\n",
    "            self.manager.swap(kwargs.get('index1', 0), kwargs.get('index2', 1))\n",
    "        elif operation == 'move_up':\n",
    "            self.manager.move_up(kwargs.get('index', 0))\n",
    "        elif operation == 'move_down':\n",
    "            self.manager.move_down(kwargs.get('index', 0))\n",
    "        elif operation == 'move_to':\n",
    "            self.manager.move_to_position(kwargs.get('from_index', 0), kwargs.get('to_index', 0))\n",
    "        elif operation == 'reverse':\n",
    "            self.manager.reverse()\n",
    "        elif operation == 'sort_time':\n",
    "            self.manager.sort_by_timestamp(kwargs.get('ascending', True))\n",
    "        elif operation == 'interleave':\n",
    "            self.manager.interleave_user_assistant()\n",
    "        else:\n",
    "            return {'status': 'error', 'message': f'Unknown operation: {operation}'}\n",
    "        \n",
    "        # Применяем изменения обратно\n",
    "        new_json = self.manager.to_json()\n",
    "        self.compressor.load_messages_json(new_json)\n",
    "        \n",
    "        return {\n",
    "            'status': 'ok',\n",
    "            'operation': operation,\n",
    "            'new_order': [{'index': i, 'role': m.role, 'preview': m.content[:40]} \n",
    "                          for i, m in enumerate(self.compressor.messages)]\n",
    "        }\n",
    "    \n",
    "    def preview(self):\n",
    "        \"\"\"Предпросмотр\"\"\"\n",
    "        self.manager.load_list([m.to_dict() for m in self.compressor.messages])\n",
    "        self.manager.preview()\n",
    "\n",
    "\n",
    "# === ДЕМОНСТРАЦИЯ API ===\n",
    "\n",
    "api = ChatAPI(max_context_tokens=2000, keep_recent_messages=2)\n",
    "\n",
    "# Симулируем диалог\n",
    "api.send_message(\"Расскажи про RAG системы\")\n",
    "api.send_message(\"RAG (Retrieval-Augmented Generation) — это метод, объединяющий поиск релевантных документов с генерацией ответа.\", role='assistant')\n",
    "api.send_message(\"А как работает сжатие контекста?\")\n",
    "api.send_message(\"Сжатие контекста уменьшает количество токенов, сохраняя семантику через различные методы: суммаризацию, удаление redundancy, выбор ключевых фраз.\", role='assistant')\n",
    "api.send_message(\"Можно ли комбинировать RAG и compression?\")\n",
    "\n",
    "print(\"=== Текущая история ===\")\n",
    "api.preview()\n",
    "\n",
    "print(\"\\n=== Меняем местами 0 и 2 ===\")\n",
    "result = api.reorder('swap', index1=0, index2=2)\n",
    "print(json.dumps(result, ensure_ascii=False, indent=2))\n",
    "\n",
    "print(\"\\n=== После swap ===\")\n",
    "api.preview()\n",
    "\n",
    "print(\"\\n=== Контекст для нового запроса ===\")\n",
    "context = api.send_message(\"Как это применить на практике?\")\n",
    "print(f\"Статистика: {context['context']['stats']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a51def8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13.4 HTTP-ready функции (для интеграции в FastAPI/Flask)\n",
    "\n",
    "def create_chat_endpoints():\n",
    "    \"\"\"\n",
    "    Пример структуры endpoints для REST API.\n",
    "    Можно скопировать в FastAPI роутер.\n",
    "    \"\"\"\n",
    "    \n",
    "    endpoints = {\n",
    "        'POST /chat/message': {\n",
    "            'description': 'Отправить сообщение',\n",
    "            'body': {'role': 'user', 'content': 'текст сообщения'},\n",
    "            'response': {'status': 'ok', 'context': {...}}\n",
    "        },\n",
    "        'GET /chat/history': {\n",
    "            'description': 'Получить всю историю',\n",
    "            'response': [{'role': '...', 'content': '...', 'timestamp': '...'}]\n",
    "        },\n",
    "        'POST /chat/reorder': {\n",
    "            'description': 'Изменить порядок сообщений',\n",
    "            'body': {'operation': 'swap|move_up|move_down|move_to|reverse', 'params': {...}},\n",
    "            'response': {'status': 'ok', 'new_order': [...]}\n",
    "        },\n",
    "        'GET /chat/export': {\n",
    "            'description': 'Экспорт в JSON',\n",
    "            'response': '[...]'\n",
    "        },\n",
    "        'POST /chat/import': {\n",
    "            'description': 'Импорт из JSON',\n",
    "            'body': {'messages': [...]},\n",
    "            'response': {'status': 'ok', 'count': N}\n",
    "        },\n",
    "        'POST /chat/compress': {\n",
    "            'description': 'Принудительное сжатие всех сообщений',\n",
    "            'response': {'status': 'ok', 'tokens_saved': N}\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return endpoints\n",
    "\n",
    "\n",
    "# Пример кода для FastAPI (можно скопировать в отдельный файл)\n",
    "fastapi_example = '''\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Инициализация глобального чата (в production использовать DI)\n",
    "chat_api = ChatAPI(max_context_tokens=4000, keep_recent_messages=3)\n",
    "\n",
    "class MessageRequest(BaseModel):\n",
    "    role: str = \"user\"\n",
    "    content: str\n",
    "\n",
    "class ReorderRequest(BaseModel):\n",
    "    operation: str\n",
    "    index: Optional[int] = None\n",
    "    index1: Optional[int] = None\n",
    "    index2: Optional[int] = None\n",
    "    from_index: Optional[int] = None\n",
    "    to_index: Optional[int] = None\n",
    "\n",
    "@app.post(\"/chat/message\")\n",
    "async def send_message(req: MessageRequest):\n",
    "    result = chat_api.send_message(req.content, req.role)\n",
    "    return result\n",
    "\n",
    "@app.get(\"/chat/history\")\n",
    "async def get_history():\n",
    "    return chat_api.get_history()\n",
    "\n",
    "@app.post(\"/chat/reorder\")\n",
    "async def reorder_messages(req: ReorderRequest):\n",
    "    params = req.dict(exclude={\"operation\"}, exclude_none=True)\n",
    "    result = chat_api.reorder(req.operation, **params)\n",
    "    return result\n",
    "\n",
    "@app.get(\"/chat/export\")\n",
    "async def export_chat():\n",
    "    return {\"json\": chat_api.export_json()}\n",
    "\n",
    "@app.post(\"/chat/import\")\n",
    "async def import_chat(messages: List[Dict]):\n",
    "    chat_api.import_json(json.dumps(messages))\n",
    "    return {\"status\": \"ok\", \"count\": len(messages)}\n",
    "'''\n",
    "\n",
    "print(\"=== Доступные API endpoints ===\")\n",
    "for endpoint, info in create_chat_endpoints().items():\n",
    "    print(f\"\\n{endpoint}\")\n",
    "    print(f\"  → {info['description']}\")\n",
    "\n",
    "print(\"\\n=== FastAPI код сохранён в переменную `fastapi_example` ===\")\n",
    "print(\"Скопируйте в отдельный файл (например, chat_api.py) для использования.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
